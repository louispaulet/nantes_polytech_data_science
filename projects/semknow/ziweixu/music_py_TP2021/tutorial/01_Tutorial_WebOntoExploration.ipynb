{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Ontology Exploration from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will do the \"Hello World\" of ontology learning: selecting significant terms and exploring their relations from web knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this section you will:\n",
    "* Understand how to pre-process the text before further training\n",
    "* Use the spaCy API to deal with textual information and prepare terms for training\n",
    "* Create a simple feature representation to perform term clustering\n",
    "* Select the significant terms from term clusters\n",
    "* Use Sparql query to catch knowledge of from a knowledge base DBpedia\n",
    "* Observe the performance of knowledge acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Relation Discovery from External Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation discovery from external knowledge base, which asks a program to acquire the relation knowledge of a certain term or acquire more terms connecting by a certain term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution: SPARQL query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPARQL query excels at solving problems where the program needs to query to a knowledge base. By indicating the query subject, and extracting the knowledge from the returns, the relation knowledge could be discovered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Music Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided [Music Corpus](https://competitions.codalab.org/competitions/17119#learn_the_details-terms_and_conditions) is a concatenation of several music-specific corpora, i.e., music biographies from Last.fm contained in ELMD 2.0 , the music branch from Wikipedia, and a corpus of album customer reviews from Amazon. Specifically, it is the 100M-word corpus including Amazon reviews, music biographies and Wikipedia pages about theory and music genres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have random selected 1000 documents for experiments, which is stored in [df_select_1000doc.csv](./df_select_1000doc.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy \n",
    "from spacy.lang.en.stop_words import STOP_WORDS #| version>2.0\n",
    "import requests\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import cluster\n",
    "\n",
    "# pip install SPARQLWrapper\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load music corpus\n",
    "path_here = os.getcwd()\n",
    "df_select_doc = pd.read_csv(path_here +'/df_select_1000doc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing text into noun phrases(NPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognize those NPs (require many time for 1000 docs, here we use only the top 100 docs to shorten execution time)\n",
    "nps_doc = []\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "for text in df_select_doc['text'][:100]:\n",
    "    np_doc = []\n",
    "    doc = nlp(text) \n",
    "    for chunk in doc.noun_chunks:\n",
    "        np_doc.append((chunk.text, chunk.root.text, chunk.root.dep_))\n",
    "    nps_doc.append(np_doc)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire the original NPs\n",
    "source_nps_doc = []\n",
    "\n",
    "for np_doc in nps_doc: \n",
    "    tmp = []\n",
    "    for np1, root, dep in np_doc:\n",
    "        tmp.append(np1)\n",
    "    source_nps_doc.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out stopwords\n",
    "source_nps_doc_v2 = []\n",
    "\n",
    "for file in source_nps_doc:\n",
    "    file_temp = []\n",
    "    for nps in file:\n",
    "        if nps.lower() not in STOP_WORDS:\n",
    "            file_temp.append(nps)\n",
    "    source_nps_doc_v2.append(file_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing term clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a feature representation for NPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use word2vec algorithm to get the feature vectors of NPs\n",
    "w2v_model = Word2Vec(source_nps_doc_v2, min_count=5) # Ignores all words with total frequency lower than 5.\n",
    "w2v_feature = w2v_model.wv.vectors \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering NPs based on the feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=10) # here we set the number of cluster is 10\n",
    "kmeans.fit(w2v_feature)\n",
    "\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the significant terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the top-n frequent terms from each cluster\n",
    "\n",
    "def topN_NPs_per_clt(LABELS, W2V_NPS, W2V_NPS_CT, TOPN_NMB):\n",
    "    topN_NPs_km = []\n",
    "    \n",
    "    for k in range(10):\n",
    "        inx = np.where(LABELS == k)[0]\n",
    "        \n",
    "        w2v_NPs_t =  W2V_NPS[inx]      \n",
    "        w2v_NPs_ct_t = W2V_NPS_CT[inx] \n",
    "        w2v_NPs_ct_dict_t = dict(zip(w2v_NPs_t, w2v_NPs_ct_t))\n",
    "        \n",
    "        topN_NPs_km.append(Counter(w2v_NPs_ct_dict_t).most_common()[:TOPN_NMB])\n",
    "    \n",
    "    return topN_NPs_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the the top-10 frequent words in each cluster\n",
    "\n",
    "w2v_NPs = np.array(list(w2v_model.wv.vocab.keys())) # NPs \n",
    "w2v_NPs_ct = np.array([j.count for i, j in w2v_model.wv.vocab.items()]) # NPs' word count\n",
    "topN_NPs_km = topN_NPs_per_clt(labels, w2v_NPs, w2v_NPs_ct, 10)\n",
    "\n",
    "topN_NPs_km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering NPs'relations from DBpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing whethter the NPs exist in DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN_NPs = [j[0] for i in topN_NPs_km for j in i]\n",
    "topN_ct = [j[1] for i in topN_NPs_km for j in i]\n",
    "topN_cl = [inx_i for inx_i, i in enumerate(topN_NPs_km) for j in i]\n",
    "\n",
    "nps_exist_text_topN = []\n",
    "nps_exist_link_topN = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [*openSearch* API](https://www.mediawiki.org/wiki/API:Opensearch) to match NPs to the existing string of DBpedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "for nps in topN_NPs:\n",
    "    print(nps)\n",
    "    PARAMS = {\n",
    "        \"action\": \"opensearch\",\n",
    "        \"namespace\": \"0\",\n",
    "        \"search\": nps ,\n",
    "        \"limit\": \"5\",\n",
    "        \"profile\" : \"fuzzy\",\n",
    "        \"format\": \"json\"}\n",
    "    \n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()  \n",
    "    \n",
    "    if len(DATA[1]) == 0:\n",
    "        nps_exist_text_topN.append([None])\n",
    "        nps_exist_link_topN.append([None])\n",
    "    else:\n",
    "        nps_exist_text_topN.append(DATA[1])\n",
    "        nps_exist_link_topN.append(DATA[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the results into a dataframe\n",
    "df_nps_exist = pd.DataFrame()\n",
    "\n",
    "df_nps_exist['sourceNPs_counts'] = topN_ct\n",
    "df_nps_exist['sourceNPs_cluster'] = topN_cl\n",
    "df_nps_exist['sourceNPs'] = topN_NPs\n",
    "df_nps_exist['WikiMatchedNPs'] = [i[0] for i in nps_exist_text_topN]  # extract the top one candidates\n",
    "df_nps_exist['WikiMatchedLink'] = [i[0] for i in nps_exist_link_topN]   # extract the top one candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering the knowledge if the NPs exist in DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the recognized NPs and get the link ID of hypernym relation\n",
    "np_query_li = ['_'.join(str(nps).split()) for nps in df_nps_exist.WikiMatchedNPs]\n",
    "rel_query_li = ['<http://purl.org/linguistics/gold/hypernym>']\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " None exist\n",
      " None exist\n",
      " None exist\n",
      " None exist\n"
     ]
    }
   ],
   "source": [
    "# query the OBJECT with given subject\n",
    "NPs_AsSub_O = [None] * len(np_query_li)\n",
    "\n",
    "for inx_np_query, np_query in enumerate(np_query_li):\n",
    "    subj_qr = np_query\n",
    "    obj_qr = np_query\n",
    "    rel_qr = rel_query_li[0]\n",
    "    \n",
    "    NPs_AsSub_O_t = []\n",
    "    \n",
    "    try:     \n",
    "        # query the OBJECT with given subject\n",
    "        sparql.setQuery(\"\"\"\n",
    "                PREFIX dbp: <http://dbpedia.org/resource/>\n",
    "                PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "                SELECT distinct ?y ?string where {\n",
    "                   dbp:\"\"\"+subj_qr+\" \"+rel_qr+\"\"\"  ?y .\n",
    "                   OPTIONAL {?y rdfs:label ?string . FILTER (lang(?string) = 'en') }}\n",
    "        \"\"\")\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            print('______')\n",
    "            print(np_query)\n",
    "            print(result[\"string\"][\"value\"])\n",
    "            NPs_AsSub_O_t.append(result[\"string\"][\"value\"])\n",
    "             \n",
    "    except:\n",
    "        # to exclude the bad formats errors\n",
    "        print(' None exist')\n",
    "    \n",
    "    NPs_AsSub_O[inx_np_query] = NPs_AsSub_O_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the SUBJECT with given object\n",
    "NPs_AsObj_S = [None] * len(np_query_li)\n",
    "\n",
    "for inx_np_query, np_query in enumerate(np_query_li):\n",
    "    subj_qr = np_query\n",
    "    obj_qr = np_query\n",
    "    rel_qr = rel_query_li[0]\n",
    "\n",
    "    NPs_AsObj_S_t = []\n",
    "    \n",
    "    \n",
    "    try:     \n",
    "        # query the SUBJECT with given object\n",
    "        sparql.setQuery(\"\"\"\n",
    "                PREFIX dbp: <http://dbpedia.org/resource/>\n",
    "                PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "                SELECT distinct ?y ?string  where {\n",
    "                   ?y \"\"\"+rel_qr+\"\"\" dbp:\"\"\"+obj_qr+\"\"\".\n",
    "                   OPTIONAL {?y rdfs:label ?string . FILTER (lang(?string) = 'en') }}\n",
    "        \"\"\")\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            print('______')\n",
    "            print(np_query)\n",
    "            print(result[\"string\"][\"value\"])\n",
    "            NPs_AsObj_S_t.append(result[\"string\"][\"value\"])\n",
    "             \n",
    "    except:\n",
    "        # to exclude the bad formats errors\n",
    "        print(' None exist')\n",
    "    \n",
    "    NPs_AsObj_S[inx_np_query] = NPs_AsObj_S_t  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished! Hope you enjoy this lab tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
