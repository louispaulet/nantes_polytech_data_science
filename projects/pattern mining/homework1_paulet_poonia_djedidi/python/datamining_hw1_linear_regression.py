# -*- coding: utf-8 -*-
"""DataMining HW1 Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bEQxYtpw-6ccrCyL5MFIQaD43atJBW8o

First we load the libraries we will need for reading and formatting the input data (numpy for the matrix structure, train_test_split from sklearn to split the data into test and training sets). 
Then we load the linear model to perform the linear regression algorithm. We wil also need some validation libraries to perform validation (like RÂ² and MSE)
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

"""We then load the data. This dataset comes from the linear regression exercise from the "Classification and Representation Learning" course.
(From here  : http://extradoc.univ-nantes.fr/mod/resource/view.php?id=54906 )
"""

#load the data set
data = np.loadtxt('polynome.data')

X = data[:, 0: -1]
Y = data[:, -1]

"""We then split the data into a training and testing dataset"""

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)

"""We load the model "LinearRegression" and fit the hypothesis with the training data.
We then display some predictions.
"""

# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(X_train, y_train)

# Make predictions using the testing set
y_pred = regr.predict(X_test)
y_pred

# Commented out IPython magic to ensure Python compatibility.
# Print the hypothesis coefficients
print('Coefficients: \n', regr.coef_)
# The mean squared error
print('Mean squared error: %.2f'
#       % mean_squared_error(y_test, y_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#       % r2_score(y_test, y_pred))

"""We can plot the data : red dots are the training set, black dots are the testing set, and the blue line is our hypothesis.
The simple linear regression model is not complex enough to predict new points with accuracy, we should use a polynomial regression.
"""

# Plot outputs
X_test, y_test

plt.scatter(X_test, y_test,  color='black')
plt.scatter(X_train, y_train,  color='red')
plt.plot(X_test, y_pred, color='blue', linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()